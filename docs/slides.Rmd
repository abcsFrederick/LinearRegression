---
title: "Linear Regression"
author: "Randy Johnson"
date: "1/7/2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(MASS)      # for parallel coordinates plots
library(tidyverse) # for data wrangling etc...
library(cowplot)   # for nicer ggplot defaults
library(animation) # for animated gifs
library(car)       # for linear regression diagnostics
library(broom)     # for `augment()`
library(knitr)     # for `kable()`
library(kableExtra)# for additional kable formatting options

# see my colorblind_palettes.R gist at https://gist.github.com/johnsonra/89e06c1089471c8055e8d8f8af20955e
cbpalette <- c("#016E82", "#8115A0", "#005AC8", "#00A0FA", "#F978FA", "#18D2DC", "#AA093C", "#FA7850", 
               "#10B45A", "#F0F031", "#A1FA82", "#FAE6BE")
```

# Overview

- Least squares regression intuition
- Assumptions
    - What are they?
    - Causes of violations of assumptions
    - Consequences
    - Model diagnostics
- Transformations

```{r, include=FALSE}
########################## Least Squares Regression Intuition ##########################
```

# Least Squares Intuition

```{r intuition data, echo=FALSE}
p.platensis <- matrix(c(5.38,	29,
                        7.36,	23,
                        6.13,	22,
                        4.75,	20,
                        8.10,	25,
                        8.62,	25,
                        6.30,	17,
                        7.44,	24,
                        7.26,	20,
                        7.17,	27,
                        7.78,	24,
                        6.23,	21,
                        5.42,	22,
                        7.87,	22,
                        5.25,	23,
                        7.37,	35,
                        8.01,	27,
                        4.92,	23,
                        7.03,	25,
                        6.45,	24,
                        5.06,	19,
                        6.72,	21,
                        7.00,	20,
                        9.39,	33,
                        6.49,	17,
                        6.34,	21,
                        6.16,	25,
                        5.74,	22), 
                      byrow = TRUE, ncol = 2, dimnames = list(NULL, c('mg', 'eggs'))) %>%
               as_data_frame()

# calculate regression line
regLine <- lm(eggs ~ mg, data = p.platensis) %>%
           coefficients()

##### take a look at the data #####
ggplot(p.platensis, aes(mg, eggs)) + 
    geom_point() +
    geom_abline(slope = regLine[2], intercept = regLine[1])
```

Sand flea data from [McDonald, J.H. 1989. Selection component analysis of the Mpi locus in the amphipod Platorchestia platensis. Heredity 62: 243-249](https://www.nature.com/articles/hdy198934).

See the [Handbook of Biological Statistics](http://www.biostathandbook.com/linearregression.html) for additional examples.

# Least Squares Intuition

```{r with error terms, echo=FALSE}
# Function to calculate predicted value and error terms
#' @param m the slope of the line
#' @param b the intercept of the line
#' @param dat the data frame containing points the line is supposed to fit
#' @return the modified data frame with two additional values added: pred (predictions based on ab) and err (the error terms)
plotLine <- function(m, b, dat)
{
    tmp <- mutate(dat,
                  pred = m*mg + b,
                  error = abs(pred - eggs))
    
    # color of the sum of squared errors bar                           
    err <- sum(tmp$error^2)
                                                         # red ranges from
    red <- ifelse(err < 428,                             #   0 to
                 0,                                      #   0 to
                 (err - 428)/129 * (250/255))            # 250
                  
                                                         # green ranges from
    grn <- ifelse(err < 428,                             #   0 to
                 (err - 362.8)/66 * (160/255),           # 160 to
                 (557 - err)/129 * ( 40/255) + 120/255)  # 120
                  
                                                         # blue ranges from
    blu <- ifelse(err < 428,                             #   0 to
                 (err - 362.8)/66 * (250/255),           # 250 to
                 (557 - err)/129 * (170/255) + 80/255)   #  80
                  
    col <- try(rgb(red, grn, blu))
    if(class(col) == 'try-error')
        stop(paste0('m=', m, ', and b=', b, " are causing an error"))

    # generate figure        
    g <- ggplot(tmp, aes(mg, eggs)) +

         geom_segment(aes(xend = mg, yend = pred, color = sqrt(error))) +
         scale_color_gradientn(colours = c('black', cbpalette[c(4,8)])) +
         theme(legend.position = 'none') +
    
         geom_point() +
    
         geom_abline(slope = m, intercept = b) +
        
         geom_segment(aes(x = 10, xend = 10, y = 17, yend = (err-300)/350*13 + 17), size = 1,
                      color = col) +
    
         geom_text(aes(label = "Sum of Squared Errors", x = 10, y = 17), angle = 90, vjust = 2, hjust = 0)
    
    # plot figure
    print(g)
    invisible(g)
}

g <- plotLine(regLine[2], regLine[1], p.platensis)
```

# Least Squares Intuition

```{r intuition video, include=FALSE}
width <- knitr::opts_chunk$get("dpi") * knitr::opts_chunk$get("fig.width") * 1.15
height <- knitr::opts_chunk$get("dpi") * knitr::opts_chunk$get("fig.height")

if(!file.exists('intuitionVideo.gif')) # this takes a few seconds to render - don't recreate unless we really want to
    saveGIF(map2(c(seq(from = regLine[2], to = 0.2, length = 90), # perturb the line
                   seq(from = 0.2, to = regLine[2], length = 10)),# snap back to regression line
                 c(seq(from = regLine[1], to = 20, length = 90),  # perturb
                   seq(from = 20, to = regLine[1], length = 10)), # snap back
                 ~ plotLine(.x, .y, dat = p.platensis)), 
            movie.name = "intuitionVideo.gif", interval = 0.1, autobrowse = FALSE,
            ani.width = width, ani.height = height)
```

![](intuitionVideo.gif)

# Model Notation

eggs = β~0~ + β~1~ mg + ε

```{r model notation}
g
```

# Model Output

eggs = β~0~ + β~1~ mg + ε

```{r model output}
pp0 <- lm(eggs ~ mg, data = p.platensis)

summary(pp0) %>%
    capture.output() %>%
    `[`(8:14) %>%
    cat(sep = '\n')
```

eggs = ```r round(regLine[1], 1)``` + ```r round(regLine[2], 1)```*mg + ε


```{r, include=FALSE}
############################# Assumptions ##############################

########## Assumption 1 - MVN residuals ##########
```

# Assumption 1: Error terms are normally distributed

Two ways to test this assumption:

* Shapiro-Wilks test (quantitative, but sensitive to outliers)
* QQ-plot (graphical test)

```{r shapiro-wilks}
with(augment(pp0), shapiro.test(.std.resid))
```

* Null hypothesis: the error terms are normally distributed.
* Alternate hypothesis: we are violating this assumption.
    * In this case, large p-values are good news.
    * Probably won't reject the null hypothesis, but we should exercise caution.

See my code and `?shapiro.test` for more details.

# Assumption 1: Error terms are normally distributed

* Check for outliers:

```{r outlierTest}
outlierTest(pp0)
```

* Null hypothesis: the data point is not an outlier.
    * Again, large p-values are good news.
    * Result is borderline significant, so we will evaluate this a little closer.

See my code and `?outlierTest` from the `car` package for more details.

# Influence

```{r influence1}
# Function to calculate predicted value and error terms
#' @param x x position
#' @param y y position
#' @param dat the data frame containing the rest of the data set
plotLine <- function(x, y, dat)
{
    dat$mg[24] <- x
    dat$eggs[24] <- y
    
    mdl <- lm(eggs ~ mg, data = dat)
    
    g <- ggplot(dat[-24,], aes(mg, eggs)) +
         geom_point() +
         #geom_smooth(method = 'lm', se = FALSE) +
        
         ylim(c(17, 35)) +
         xlim(c(4.7, 9.4)) +
        
         geom_abline(slope = regLine[2], intercept = regLine[1], linetype = 'dashed') +
         geom_abline(slope = coef(mdl)[2], intercept = coef(mdl)[1], color = 'blue') +
        
         geom_point(data = data_frame(x = x, y = y), aes(x, y), color = 'red')
    
    print(g)
}

plotLine(9.39, 33, p.platensis)
```

# Influence

```{r influence, include = FALSE}
width <- knitr::opts_chunk$get("dpi") * knitr::opts_chunk$get("fig.width")
height <- knitr::opts_chunk$get("dpi") * knitr::opts_chunk$get("fig.height")

if(!file.exists('influenceVideo.gif')) # this takes a few seconds to render - don't recreate unless we really want to
    saveGIF(map2(c(rep(9.39, 6),                             # (x) up to top right corner
                   seq(from = 9.39, to = 4.75, length = 50), # (x) over to top left corner
                   rep(4.75, 50),                            # (x) down to bottom left corner
                   seq(from = 4.75, to = 9.39, length = 50), # (x) over to bottom right corner
                   rep(9.39, 44)),                           # (x) back to starting point
                 c(seq(from = 33, to = 35, length = 6),      # (y) up to top right corner
                   rep(35, 50),                              # (y) over to top left corner
                   seq(from = 35, to = 17, length = 50),     # (y) down to bottom left corner
                   rep(17, 50),                              # (y) over to bottom right corner
                   seq(from = 17, to = 33, length = 44)),    # (y) back to starting point
                 plotLine, dat = p.platensis), 
            movie.name = "influenceVideo.gif", interval = 0.1, autobrowse = FALSE,
            ani.width = width, ani.height = height)
```

![](influenceVideo.gif)

# Influence

```{r influencePlot}
par(cex.lab = 1.5, cex.axis = 1.25)
capture.output(influencePlot(pp0), file = '/dev/null')
```

* Studentized Residuals (y-axis): The number of standard deviations away from the trend line for each data point.
* Hat-Values (x-axis): A measure of how much the coefficients change (i.e. slope and intercept) if you exclude each data point. Vertical reference lines are drawn at twice and three times the average hat value.
* Cook's Distance (radius of bubbles): A measure of how much the error terms would change when excluding each data point.

We see our borderline significant outlier (#16) has very little influence over the coefficients, but it does result in a change in the error terms.

    * The trend line is not changed much if we exclude this data point from the analysis.
    * It does shift the line slightly up, which results in a slightly larger estimate for all data points.
    
The 24th data point is also highlighted here. It has a larger residual (not an outlier) and is on the edge of the distribution. This gives it increased leverage and influence.

See my code and `?influencePlot` from the `car` package for more details.

# Assumption 1: Error terms are normally distributed

```{r qqPlot}
par(cex.lab = 1.5, cex.axis = 1.25)
qqPlot(pp0, ylab = 'Studentized Residuals')
```

* Studentized Residuals (y-axis): The number of standard deviations away from the trend line for each data point. Error terms are ordered from lest (bottom left corner) to greatest (top right corner).
* t Quantiles (x-axis): The theoretical size of the residuals if the error terms perfectly followed a normal distribution.
* Solid line: If the error terms were perfectly normally distributed, all of the data points would lie along this line.
* Dashed lines: This region shows where we can expect the observed error terms to fall, given sampling variability.

We see our borderline outlier in the top right corner is larger than we would expect it to be, but not quite outside of the confidence region within which we can expect our error terms to fall.

See my code and `?qqPlot` from the `car` package for more details.

```{r, include = FALSE}
########## Assumption 2 - Linearity ##########
```

# Assumption 2: Linear Relationship

size = β~0~ + β~1~ length + ε

size = β~0~ + β~1~ length + β~2~ length^2^ + ε

```{r ashton data}
# I used a ruler to guestimate these values from Figure 2 of Ashton 2007 referenced below - probably not terribly accurate, but sufficient for this example
ashton <- data_frame(size = c(3, 2, 7, 11, 12, 10, 8, 9, 10, 13, 9, 7, 6, 13, 8, 2), # clutch size determined by x-ray imaging
                     length = c(284, 290, 290, 298, 299, 302, 306, 309, 310, 311, 
                                317, 317, 320, 323, 334, 334)) %>%                   # female carapace length in mm
          mutate(length2 = length^2)

ggplot(ashton, aes(length, size)) + 
    geom_point() + 
    geom_smooth(method = 'lm', se = FALSE) +
    geom_smooth(method = 'lm', se = FALSE, formula = y ~ x + identity(x^2), color = 'green4') +
    ylab("Clutch Size") +
    xlab("Carapace Length (mm)")
```

Data from [Ashton, K.G., R.L. Burke, and J.N. Layne. 2007. Geographic variation in body and clutch size of gopher tortoises. Copeia 2007: 355-363](http://www.asihcopeiaonline.org/doi/abs/10.1643/0045-8511(2007)7%5B355:GVIBAC%5D2.0.CO%3B2?code=asih-site)

# Assumption 2: Linear Relationship

size = β~0~ + β~1~ length + ε

```{r linear candidates1}
ash0 <- lm(size ~ length, data = ashton)
    
summary(ash0) %>%
    capture.output() %>%
    `[`(8:12) %>%
    cat(sep = '\n')
```

size = β~0~ + β~1~ length + β~2~ length^2^ + ε

```{r linear candidates2}
ash1 <- lm(size ~ length + length2, data = ashton)

summary(ash1) %>%
    capture.output() %>%
    `[`(8:15) %>%
    cat(sep = '\n')
```

# Component Residual Plots

size = β~0~ + β~1~ length + ε

```{r crPlots1}
crPlots(ash0)
```

# Component Residual Plots

size = β~0~ + β~1~ length + β~2~ length^2^ + ε

```{r crPlots2}
crPlots(ash1)
```

# Assumption 2: Linear Relationship

How might you analyze data with a non-linear relationship?

* Transform the data to make it linear
* Add terms to the model (e.g. to model polynomials)
* Non-linear regression

```{r, include = FALSE}
########## Assumption 3 - Little/No Multicollinearity ##########
```

# Assumption 3: No/Little Multicollinearity

```{r mcol1a, echo = FALSE, warning = FALSE, message = FALSE}
# Download the data from https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/downloads/data.csv/2
# and place in ../data/bc.csv

(bc <- read_csv('../data/bc.csv') %>%
       mutate(metastatic = diagnosis == "M") %>%
       select(-id, -X33, -diagnosis))
```

Breast cancer data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). 

See this [kaggle notebook by Mike M. Lee](https://www.kaggle.com/leemun1/predicting-breast-cancer-logistic-regression) for a more thorough exploration of this dataset.

# Assumption 3: No/Little Multicollinearity

```{r vif, echo = TRUE}
# check for multicollinearity
glm(metastatic ~ radius_mean + perimeter_mean + area_mean + texture_mean + smoothness_mean + concavity_mean, data = bc) %>%
    vif()
```

# Assumption 3: No/Little Multicollinearity

```{r Demensionality Reduction, include = FALSE}
# Reduce dimensionality with Principal Components Analysis
pca <- select(bc, -metastatic) %>%
       prcomp(scale. = TRUE)

# We can capture 80% of the variability of 30 variables with 11 PCs
sum((cumsum(pca$sdev) / sum(pca$sdev)) < .8)

# pull out principal components
bc_dr <- cbind(bc, predict(pca, bc)[,1:11])

# parallel coordinates plot
pcs <- paste0('PC', 1:11)

clrs <- mutate(bc_dr,
               centr1 = radius_mean - min(radius_mean),
               scale1 = centr1 / max(centr1),
               col1 = rgb(scale1, scale1 * 165/255, 0), # scale from black to orange
               centr2 = perimeter_mean - min(perimeter_mean),
               scale2 = centr2 / max(centr2),
               col2 = rgb(sqrt(scale1), sqrt(scale2 * 165/255), 0, .5), # scale from black to orange
               centr3 = area_mean - min(area_mean),
               scale3 = centr3 / max(centr3),
               col3 = rgb(scale1, scale3 * 165/255, 0)) # scale from black to orange
parcoord(bc_dr[,pcs], col = clrs$col3)

ggplot(bc_dr, aes(PC1, PC2)) +
    geom_point() +
    scale_color_manual(values = clrs$col2)

with(bc_dr, plot(PC1, PC2, col = clrs$col2, pch = 19, bty = 'l'))
```

Options:

* Reduce the number of variables in the model
    * Pro: model statistics will behave better
    * Con: might loose some predictive power
* Reduce dimensionality using PCA
    * Pro: can include information from all variables
    * Con: interpretation of model statistics will be hard
* Leave everything in the model
    * Pro: can include all variables
    * Con: model statistics will behave badly
    * Con: too many variables will cause overfitting
* Penalized regression
* Random forests

```{r, include = FALSE}
########## Assumption 4 - No Autocorrelation ##########
```

# Assumption 4: No Autocorrelation

```{r auto1, message=FALSE}
# measles incidence in Baltimore between 1948 and 1987
measles <- read_delim(url("http://ms.mcmaster.ca/~bolker/measdata/baltmeas.dat"), ' ', col_names = FALSE) %>%
           rename(dt = X1, Incidence = X2)

ggplot(measles, aes(dt, Incidence)) +
    geom_line() +
    xlab("Year") +
    ylab("Measles Incidence")
```

Data on measles incidence in Baltimore were collected from published reports by various people and hosted by Ben Bolker at [McMaster University](http://ms.mcmaster.ca/~bolker/measdata.html).

# Assumption 4: No Autocorrelation

```{r auto3, echo=TRUE}
lm(Incidence ~ dt, data = measles) %>%
    durbinWatsonTest()
```


```{r, include = FALSE}
########## Assumption 5 - Errors are Homoscedastic ##########
```

# Assumption 5: Homoscedasticity

```{r homo1, message=FALSE}
# height <- read_csv('../data/NHIS 2007 data.csv') %>%
#           filter(SEX == 2 & weight < 600 & height < 80)
#Height/weight data from the CDC's [National Health Interview Survey](https://www.cdc.gov/nchs/nhis/index.htm).

height <- read_delim(url("https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv"), ';') %>%
          filter(male == 0)

ggplot(height, aes(height, weight)) +
    geom_jitter()

mdl1 <- lm(weight ~ height, data = height)
# mdl2 <- lm(weight ~ height + age, data = height)
```

These are [partial census data](https://github.com/rmcelreath/rethinking/blob/master/data/Howell1.csv) for females in the Dobe area !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s.


# Assumption 5: Homoscedasticity

```{r homo2, message=FALSE}
spreadLevelPlot(mdl1)
ncvTest(mdl1)
```

# Assumption 5: Homoscedasticity

How might you model heteroscedastic data?

* Transform the data (usually log or power transformation)
* Add terms to the model
* Weighted regression analysis

```{r, include=FALSE}
############################# Transformations ##############################
```

# Transformations

Many of the problems we encounter in linear regression stem from model choice.

Two most common transormations:

* Power transformations
* Log/exponential transformations

# Power Transformations

Original model: y ~ β~0~ + β~1~x + ε

With power transformation: y ~ β~0~ + β~1~x^2^ + ε

```{r x2}
n <- 100

set.seed(923843)
pow2 <- data_frame(x = runif(n)*4,
                   x2 = x^2,
                   y = x2 + rnorm(n),
                   x_int = map2(x, x2, ~ seq(from = .x, to = .y, len = 60)))

g <- list(ggplot(pow2, aes(x, y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE),
          ggplot(pow2, aes(x2, y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE) +
              xlab(expression(x^2)))

plot_grid(plotlist = g)
```


# Power Transformations

Original model: y ~ β~0~ + β~1~x + ε

With power transformation: y ~ β~0~ + β~1~sqrt(x) + ε

```{r x.5}
n <- 100

set.seed(923843)
pow.5 <- data_frame(x = runif(n)*100,
                   x.5 = sqrt(x),
                   y = x.5 + rnorm(n, sd = 0.5),
                   x_int = map2(x, x.5, ~ seq(from = .x, to = .y, len = 60)))

g <- list(ggplot(pow.5, aes(x, y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE),
          ggplot(pow.5, aes(x.5, y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE) +
              xlab(expression(sqrt(x))))

plot_grid(plotlist = g)
```


# Log Transformations

Original model: y ~ β~0~ + β~1~x + ε

With log transformation: y ~ β~0~ + β~1~ln(x) + ε

```{r lnx}
n <- 100

set.seed(923843)
expx <- data_frame(x = runif(n)*10,
                   expx = exp(x),
                   y = x + rnorm(n, 5),
                   x_int = map2(expx, x, ~ exp(seq(from = log(.x), to = log(.y), len = 60))))

g <- list(ggplot(expx, aes(expx, y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE) +
              xlab('x'),
          ggplot(expx, aes(x, y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE) +
              xlab('log(x)'))

plot_grid(plotlist = g)
```


# Log Transformations

Original model: y ~ β~0~ + β~1~x + ε

With log transformation: ln(y) ~ β~0~ + β~1~x + ε

```{r lny}
n <- 100

set.seed(923843)
lny <- data_frame(x = runif(n)*10,
                  expx = exp(x),
                  y = exp(x + rnorm(n, 5)),
                  y_int = map2(y, log(y), ~ exp(seq(from = log(.x), to = log(.y), len = 60))))

g <- list(ggplot(lny, aes(x, y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE),
          ggplot(lny, aes(x, log(y))) +
              geom_point() +
              geom_smooth(method = 'loess', se = FALSE))

plot_grid(plotlist = g)
```



```{r, include=FALSE}
############################# Closing ##############################
```

# Assumptions Summary

```{r assumptions}
# this is the table I want, but I also want some better formatting
# | Assumption                  | Cause                | Consequence                 | Diagnosis                 |
# |:----------------------------|:---------------------|:----------------------------|:--------------------------|
# | Linear Relationship         | Bad model            | Inaccurate predictions      | `car::crPlots()`          |
# | Multivariate Normality      | Bad model            | Incorrect statistics        | `car::qqPlot()`           |
# |                             | Noisy data           | (p-values / CIs)            | `shapiro.test(residuals)` |
# | No/Little Multicollinearity | Correlated variables | Unstable model coefficients | `car::vif()`              |
# | No Autocorrelation          | Non-independent data | Inefficient estimators      | `car::durbinWatsonTest()` |
# | Homoscedasticity            | Bad model            | Incorrect statistics        | `car::ncvTest()`          |
# |                             | "Bad" data           | (p-values / CIs)            | `car::spreadLevelPlot()`  |

tab <- data_frame(Assumption = c('Linear Relationship', 'Multivariate Normality', 'Multivariate Normality', 
                                 'No/Little Multicollinearity', 'No Autocorrelation', 'Homoscedasticity', 'Homoscedasticity'),
                  Cause = c('Bad model', 'Bad model', 'Noisy data', 'Correlated variables', 
                            'Non-independent data', 'Bad model', '"Bad" data'),
                  Consequence = c('Inaccurate predictions', 'Incorrect statistics', '(p-values / CIs)', 
                                  'Unstable model coefficients', 'Inefficient estimators', 'Incorrect statistics',
                                  '(p-values / CIs)'),
                  Diagnosis = c('car::crPlots()', 'car::qqPlot()', 'shapiro.test(residuals)', 'car::vif()', 
                                'car::durbinWatsonTest()', 'car::ncvTest()', 'car::spreadLevelPlot()'))

kable(tab) %>%
    column_spec(column = 4, monospace = TRUE) %>%
    collapse_rows(columns = 1, valign = 'top') %>%
    kable_styling()
```

# Thanks

Statistics for Lunch Team

* Greg Alvord
* Eckart Bindewald
* Taina Immonen
* Brian Luke
* George Nelson
* Ravi Ravichandran
* Tom Schneider
    
See [https://github.com/abcsFrederick/LinearRegression](https://github.com/abcsFrederick/LinearRegression) for code.
